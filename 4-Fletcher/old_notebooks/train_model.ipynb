{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ['THEANO_FLAGS'] = 'device=cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN0 = 'vocabulary-embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN1 = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlend = 100 #number of words from transcript\n",
    "maxlenh = 25 #number of words from title\n",
    "maxlen = maxlend + maxlenh  #total length of input\n",
    "\n",
    "rnn_size = 512\n",
    "rnn_layers = 3\n",
    "batch_norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_rnn_size = 40 if maxlend else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Parameters\n",
    "seed = 42\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0,0,0,0,0\n",
    "optimizer = 'Adam'\n",
    "LR = 1e-4\n",
    "batch_size = 64\n",
    "n_flips = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 5000\n",
    "nb_val_samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/%s.pkl'%FN0, 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "vocab_size, embedding_size = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/%s.data.pkl'%FN0, 'rb') as fp:\n",
    "    X, Y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_unknown_words = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples 5858 5858\n",
      "dimension of embedding space for words 100\n",
      "vocabulary size 40000 the last 30 words can be used as place holders for unknown/oov words\n",
      "total number of different words 203556 203556\n",
      "number of words outside vocabulary which we can substitue using glove similarity 41880\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) 121676\n"
     ]
    }
   ],
   "source": [
    "print ('number of examples',len(X),len(Y))\n",
    "print ('dimension of embedding space for words',embedding_size)\n",
    "print ('vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words)\n",
    "print ('total number of different words',len(idx2word), len(word2idx))\n",
    "print ('number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx))\n",
    "print ('number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size-1-i] = '<%d>'%i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov0 = vocab_size-nb_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5358, 5358, 500, 500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=nb_val_samples, random_state=seed)\n",
    "len(X_train), len(Y_train), len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import random,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prt(label, x):\n",
    "    print(label+':',)\n",
    "    for w in x:\n",
    "        print(idx2word[w],)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\n",
      "A\n",
      "solution\n",
      "for\n",
      "homelessness\n",
      "Community\n",
      "Based\n",
      "Problem\n",
      "Solving\n",
      "Adam\n",
      "Rideau\n",
      "TEDxTemecula\n",
      "\n",
      "D:\n",
      "[Music]\n",
      "I'd\n",
      "like\n",
      "everyone\n",
      "to\n",
      "take\n",
      "a\n",
      "minute\n",
      "and\n",
      "think\n",
      "about\n",
      "their\n",
      "day\n",
      "if\n",
      "you're\n",
      "anything\n",
      "like\n",
      "me\n",
      "you\n",
      "wake\n",
      "up\n",
      "in\n",
      "the\n",
      "morning\n",
      "take\n",
      "a\n",
      "shower\n",
      "get\n",
      "dressed\n",
      "eat\n",
      "breakfast\n",
      "drive\n",
      "to\n",
      "work\n",
      "work\n",
      "eat\n",
      "lunch\n",
      "work\n",
      "some\n",
      "more\n",
      "drive\n",
      "home\n",
      "eat\n",
      "dinner\n",
      "and\n",
      "go\n",
      "to\n",
      "sleep\n",
      "what\n",
      "if\n",
      "you\n",
      "got\n",
      "into\n",
      "a\n",
      "car\n",
      "accident\n",
      "and\n",
      "your\n",
      "car\n",
      "was\n",
      "totaled\n",
      "you'd\n",
      "Oberer\n",
      "right\n",
      "you'd\n",
      "use\n",
      "lifts\n",
      "you\n",
      "use\n",
      "a\n",
      "bus\n",
      "I'd\n",
      "still\n",
      "make\n",
      "it\n",
      "into\n",
      "work\n",
      "no\n",
      "problem\n",
      "okay\n",
      "so\n",
      "all\n",
      "the\n",
      "money\n",
      "that\n",
      "you're\n",
      "now\n",
      "using\n",
      "on\n",
      "uber\n",
      "causes\n",
      "you\n",
      "not\n",
      "to\n",
      "be\n",
      "able\n",
      "to\n",
      "pay\n",
      "for\n",
      "your\n",
      "water\n",
      "bill\n",
      "you\n",
      "can't\n",
      "take\n",
      "a\n",
      "shower\n",
      "so\n",
      "are\n",
      "you\n",
      "gonna\n",
      "be\n",
      "the\n",
      "stinky\n",
      "guy\n",
      "who\n",
      "shows\n",
      "up\n",
      "at\n",
      "lyft\n",
      "to\n",
      "go\n",
      "to\n",
      "work\n",
      "oh\n",
      "oh\n",
      "one\n",
      "more\n",
      "thing\n",
      "you\n",
      "don't\n",
      "have\n",
      "clean\n",
      "clothes\n",
      "now\n",
      "you're\n",
      "gonna\n",
      "be\n",
      "the\n",
      "stinky\n",
      "guy\n",
      "who's\n",
      "in\n",
      "dirty\n",
      "clothes\n",
      "who\n",
      "shows\n",
      "up\n",
      "on\n",
      "uber\n",
      "you\n",
      "don't\n",
      "have\n",
      "money\n",
      "you\n",
      "don't\n",
      "have\n",
      "food\n",
      "anymore\n",
      "could\n",
      "you\n",
      "even\n",
      "go\n",
      "to\n",
      "work\n",
      "I\n",
      "want\n",
      "to\n",
      "talk\n",
      "to\n",
      "you\n",
      "about\n",
      "the\n",
      "invisible\n",
      "population\n",
      "you've\n",
      "seen\n",
      "them\n",
      "they\n",
      "are\n",
      "visible\n",
      "believe\n",
      "it\n",
      "or\n",
      "not\n",
      "you've\n",
      "seen\n",
      "them\n",
      "in\n",
      "the\n",
      "park\n",
      "you've\n",
      "seen\n",
      "them\n",
      "downtown\n",
      "you've\n",
      "seen\n",
      "them\n",
      "on\n",
      "the\n",
      "off\n",
      "ramps\n",
      "to\n",
      "the\n",
      "freeway\n",
      "and\n",
      "now\n",
      "you\n",
      "all\n",
      "know\n",
      "exactly\n",
      "who\n",
      "I'm\n",
      "talking\n",
      "about\n",
      "it's\n",
      "the\n",
      "homeless\n",
      "but\n",
      "this\n",
      "is\n",
      "why\n",
      "they're\n",
      "invisible\n",
      "take\n",
      "a\n",
      "second\n",
      "and\n",
      "think\n",
      "about\n",
      "the\n",
      "last\n",
      "person\n",
      "that\n",
      "you\n",
      "saw\n",
      "that\n",
      "was\n",
      "homeless\n",
      "do\n",
      "you\n",
      "remember\n",
      "what\n",
      "they\n",
      "look\n",
      "like\n",
      "what\n",
      "color\n",
      "was\n",
      "her\n",
      "hair\n",
      "now\n",
      "what\n",
      "you\n",
      "remember\n",
      "unfortunately\n",
      "is\n",
      "that\n",
      "they\n",
      "were\n",
      "gross\n",
      "but\n",
      "their\n",
      "clothing\n",
      "was\n",
      "dirty\n",
      "that\n",
      "they\n",
      "were\n",
      "doing\n",
      "weird\n",
      "things\n",
      "that\n",
      "were\n",
      "they\n",
      "were\n",
      "huddled\n",
      "up\n",
      "in\n",
      "a\n",
      "group\n",
      "in\n",
      "a\n",
      "park\n",
      "you\n",
      "didn't\n",
      "remember\n",
      "that\n",
      "they\n",
      "were\n",
      "a\n",
      "person\n",
      "they\n",
      "became\n",
      "invisible\n",
      "to\n",
      "you\n",
      "well\n",
      "I\n",
      "want\n",
      "to\n",
      "introduce\n",
      "you\n",
      "to\n",
      "Charlotte\n",
      "when\n",
      "you\n",
      "watch\n",
      "this\n",
      "video\n",
      "you're\n",
      "gonna\n",
      "say\n",
      "this\n",
      "that\n",
      "she's\n",
      "an\n",
      "actor\n",
      "she's\n",
      "not\n",
      "an\n",
      "actor\n",
      "in\n",
      "order\n",
      "for\n",
      "Charlotte\n",
      "to\n",
      "allow\n",
      "me\n",
      "to\n",
      "film\n",
      "her\n",
      "to\n",
      "show\n",
      "you\n",
      "all\n",
      "Charlotte\n",
      "used\n",
      "the\n",
      "last\n",
      "bottle\n",
      "of\n",
      "water\n",
      "that\n",
      "she\n",
      "had\n",
      "to\n",
      "wash\n",
      "her\n",
      "face\n",
      "she\n",
      "had\n",
      "to\n",
      "get\n",
      "the\n",
      "cleanest\n",
      "dirty\n",
      "shirt\n",
      "that\n",
      "she\n",
      "had\n",
      "and\n",
      "oh\n",
      "by\n",
      "the\n",
      "way\n",
      "since\n",
      "she\n",
      "washed\n",
      "her\n",
      "face\n",
      "she\n",
      "couldn't\n",
      "wash\n",
      "her\n",
      "hair\n",
      "so\n",
      "she\n",
      "had\n",
      "to\n",
      "find\n",
      "a\n",
      "hat\n",
      "then\n",
      "she\n",
      "had\n",
      "to\n",
      "ask\n",
      "her\n",
      "friend\n",
      "she\n",
      "could\n",
      "borrow\n",
      "a\n",
      "necklace\n",
      "so\n",
      "that\n",
      "she'd\n",
      "be\n",
      "pretty\n",
      "for\n",
      "the\n",
      "video\n",
      "this\n",
      "is\n",
      "Charlotte\n",
      "it's\n",
      "just\n",
      "hard\n",
      "to\n",
      "get\n",
      "out\n",
      "once\n",
      "you\n",
      "hear\n",
      "I'm\n",
      "having\n",
      "running\n",
      "water\n",
      "all\n",
      "the\n",
      "things\n",
      "I\n",
      "used\n",
      "to\n",
      "take\n",
      "forgot\n",
      "it\n",
      "you\n",
      "know\n",
      "I\n",
      "had\n",
      "it\n",
      "nice\n",
      "in\n",
      "Orange\n",
      "County\n",
      "I\n",
      "had\n",
      "a\n",
      "job\n",
      "for\n",
      "20\n",
      "years\n",
      "I\n",
      "had\n",
      "a\n",
      "brain\n",
      "aneurysm\n",
      "I\n",
      "didn't\n",
      "have\n",
      "that\n",
      "planned\n",
      "you\n",
      "know\n",
      "the\n",
      "results\n",
      "can\n",
      "trickle\n",
      "down\n",
      "I\n",
      "got\n",
      "addicted\n",
      "to\n",
      "pain\n",
      "pills\n",
      "after\n",
      "that\n",
      "for\n",
      "a\n",
      "while\n",
      "so\n",
      "I\n",
      "have\n",
      "my\n",
      "addiction\n",
      "battles\n",
      "too\n",
      "but\n",
      "there's\n",
      "you\n",
      "know\n",
      "it's\n",
      "just\n",
      "hard\n",
      "to\n",
      "find\n",
      "food\n",
      "it's\n",
      "hard\n",
      "to\n",
      "get\n",
      "a\n",
      "job\n",
      "without\n",
      "an\n",
      "address\n",
      "without\n",
      "a\n",
      "shower\n",
      "to\n",
      "be\n",
      "clean\n",
      "to\n",
      "have\n",
      "clean\n",
      "clothes\n",
      "you\n",
      "know\n",
      "have\n",
      "a\n",
      "mailing\n",
      "address\n",
      "it's\n",
      "just\n",
      "what\n",
      "do\n",
      "you\n",
      "put\n",
      "in\n",
      "an\n",
      "application\n",
      "I\n",
      "just\n",
      "wanted\n",
      "out\n",
      "of\n",
      "here\n",
      "the\n",
      "pilot\n",
      "stuff\n",
      "I\n",
      "get\n",
      "out\n",
      "of\n",
      "a\n",
      "dumpster\n",
      "yeah\n",
      "I\n",
      "think\n",
      "a\n",
      "job\n",
      "I\n",
      "don't\n",
      "know\n",
      "if\n",
      "everybody\n",
      "out\n",
      "there\n",
      "wants\n",
      "to\n",
      "work\n",
      "I\n",
      "know\n",
      "me\n",
      "personally\n",
      "I\n",
      "prefer\n",
      "to\n",
      "work\n",
      "it\n",
      "self\n",
      "worth\n",
      "that\n",
      "you\n",
      "get\n",
      "you\n",
      "know\n",
      "cause\n",
      "your\n",
      "self-esteem\n",
      "gets\n",
      "real\n",
      "low\n",
      "out\n",
      "here\n",
      "you\n",
      "feel\n",
      "like\n",
      "you're\n",
      "nothing\n",
      "nobody\n",
      "you\n",
      "don't\n",
      "matter\n",
      "you\n",
      "know\n",
      "people\n",
      "walk\n",
      "around\n",
      "you\n",
      "like\n",
      "you're\n",
      "good\n",
      "day\n",
      "just\n",
      "on\n",
      "the\n",
      "sidewalk\n",
      "and\n",
      "pull\n",
      "their\n",
      "kids\n",
      "away\n",
      "we're\n",
      "not\n",
      "some\n",
      "people\n",
      "we're\n",
      "not\n",
      "good\n",
      "people\n",
      "out\n",
      "here\n",
      "but\n",
      "there's\n",
      "a\n",
      "lot\n",
      "of\n",
      "people\n",
      "who\n",
      "are\n",
      "good\n",
      "people\n",
      "I\n",
      "think\n",
      "I'm\n",
      "one\n",
      "of\n",
      "em\n",
      "you\n",
      "know\n",
      "my\n",
      "heart's\n",
      "in\n",
      "the\n",
      "right\n",
      "place\n",
      "that\n",
      "was\n",
      "Charlotte\n",
      "she\n",
      "had\n",
      "a\n",
      "brain\n",
      "aneurysm\n",
      "and\n",
      "now\n",
      "she's\n",
      "homeless\n",
      "there's\n",
      "three\n",
      "types\n",
      "of\n",
      "homelessness\n",
      "transitional\n",
      "believe\n",
      "it\n",
      "or\n",
      "not\n",
      "it's\n",
      "you\n",
      "and\n",
      "me\n",
      "some\n",
      "kind\n",
      "of\n",
      "medical\n",
      "catastrophe\n",
      "happens\n",
      "to\n",
      "us\n",
      "all\n",
      "the\n",
      "sudden\n",
      "we\n",
      "can't\n",
      "pay\n",
      "our\n",
      "bills\n",
      "something\n",
      "happens\n",
      "to\n",
      "our\n",
      "car\n",
      "now\n",
      "we\n",
      "have\n",
      "to\n",
      "start\n",
      "using\n",
      "money\n",
      "in\n",
      "other\n",
      "ways\n",
      "it\n",
      "could\n",
      "be\n",
      "any\n",
      "one\n",
      "of\n",
      "us\n",
      "episodic\n",
      "these\n",
      "are\n",
      "individuals\n",
      "that\n",
      "go\n",
      "in\n",
      "and\n",
      "out\n",
      "of\n",
      "homelessness\n",
      "more\n",
      "often\n",
      "than\n",
      "not\n",
      "they\n",
      "have\n",
      "behavioral\n",
      "health\n",
      "issues\n",
      "that\n",
      "need\n",
      "to\n",
      "be\n",
      "taken\n",
      "care\n",
      "of\n",
      "which\n",
      "causes\n",
      "that\n",
      "eventually\n",
      "these\n",
      "individuals\n",
      "become\n",
      "chronically\n",
      "homeless\n",
      "these\n",
      "are\n",
      "the\n",
      "individuals\n",
      "that\n",
      "really\n",
      "need\n",
      "programs\n",
      "that\n",
      "need\n",
      "help\n",
      "the\n",
      "problem\n",
      "is\n",
      "growing\n",
      "significantly\n",
      "I'm\n",
      "going\n",
      "to\n",
      "put\n",
      "a\n",
      "number\n",
      "up\n",
      "on\n",
      "the\n",
      "screen\n",
      "and\n",
      "it's\n",
      "kind\n",
      "of\n",
      "devastating\n",
      "really\n",
      "it's\n",
      "the\n",
      "increase\n",
      "of\n",
      "our\n",
      "homeless\n",
      "population\n",
      "within\n",
      "this\n",
      "city\n",
      "in\n",
      "one\n",
      "year\n",
      "but\n",
      "there's\n",
      "government\n",
      "programs\n",
      "right\n",
      "there's\n",
      "cities\n",
      "that\n",
      "are\n",
      "taking\n",
      "care\n",
      "of\n",
      "this\n",
      "our\n",
      "city\n",
      "is\n",
      "taking\n",
      "care\n",
      "of\n",
      "this\n",
      "right\n",
      "what\n",
      "can\n",
      "you\n",
      "do\n",
      "because\n",
      "obviously\n",
      "cities\n",
      "states\n",
      "the\n",
      "nation\n",
      "they're\n",
      "doing\n",
      "what\n",
      "they\n",
      "can\n",
      "but\n",
      "they're\n",
      "not\n",
      "taking\n",
      "care\n",
      "of\n",
      "it\n",
      "what's\n",
      "the\n",
      "answer\n",
      "what's\n",
      "the\n",
      "solution\n",
      "community-based\n",
      "problem-solving\n",
      "in\n",
      "times\n",
      "of\n",
      "crisis\n",
      "human\n",
      "beings\n",
      "come\n",
      "together\n",
      "look\n",
      "at\n",
      "Harvey\n",
      "look\n",
      "at\n",
      "911\n",
      "you\n",
      "had\n",
      "people\n",
      "from\n",
      "different\n",
      "states\n",
      "and\n",
      "Harvey\n",
      "coming\n",
      "in\n",
      "and\n",
      "helping\n",
      "out\n",
      "because\n",
      "mankind\n",
      "was\n",
      "in\n",
      "trouble\n",
      "guess\n",
      "what\n",
      "mankind\n",
      "is\n",
      "in\n",
      "trouble\n",
      "a\n",
      "friend\n",
      "of\n",
      "mine\n",
      "came\n",
      "and\n",
      "visited\n",
      "me\n",
      "and\n",
      "he\n",
      "was\n",
      "from\n",
      "a\n",
      "foreign\n",
      "country\n",
      "and\n",
      "he\n",
      "saw\n",
      "this\n",
      "individual\n",
      "who\n",
      "was\n",
      "homeless\n",
      "and\n",
      "it\n",
      "blew\n",
      "my\n",
      "mind\n",
      "to\n",
      "think\n",
      "about\n",
      "this\n",
      "because\n",
      "he\n",
      "gunned\n",
      "the\n",
      "invisible\n",
      "population\n",
      "you\n",
      "know\n",
      "we\n",
      "don't\n",
      "see\n",
      "him\n",
      "and\n",
      "he\n",
      "said\n",
      "how\n",
      "in\n",
      "America\n",
      "can\n",
      "you\n",
      "have\n",
      "someone\n",
      "that's\n",
      "living\n",
      "on\n",
      "the\n",
      "streets\n",
      "I\n",
      "I\n",
      "couldn't\n",
      "understand\n",
      "it\n",
      "I\n",
      "said\n",
      "what\n",
      "do\n",
      "you\n",
      "mean\n",
      "just\n",
      "look\n",
      "in\n",
      "LA\n",
      "rows\n",
      "and\n",
      "sections\n",
      "of\n",
      "people\n",
      "that\n",
      "are\n",
      "homeless\n",
      "community-based\n",
      "problem\n",
      "solving\n",
      "communities\n",
      "coming\n",
      "together\n",
      "to\n",
      "solve\n",
      "the\n",
      "issue\n",
      "you\n",
      "have\n",
      "organizations\n",
      "within\n",
      "each\n",
      "of\n",
      "your\n",
      "communities\n",
      "that\n",
      "do\n",
      "specialize\n",
      "in\n",
      "homelessness\n",
      "they\n",
      "do\n",
      "outreach\n",
      "they\n",
      "provide\n",
      "food\n",
      "sometimes\n",
      "they\n",
      "provide\n",
      "shelter\n",
      "the\n",
      "problem\n",
      "is\n",
      "is\n",
      "that\n",
      "they're\n",
      "taking\n",
      "on\n",
      "the\n",
      "problem\n",
      "as\n",
      "a\n",
      "whole\n",
      "and\n",
      "they're\n",
      "solving\n",
      "the\n",
      "problem\n",
      "okay\n",
      "they're\n",
      "not\n",
      "doing\n",
      "a\n",
      "great\n",
      "job\n",
      "they're\n",
      "doing\n",
      "what\n",
      "they\n",
      "can\n",
      "community\n",
      "based\n",
      "problem\n",
      "solving\n",
      "is\n",
      "the\n",
      "community\n",
      "coming\n",
      "together\n",
      "it's\n",
      "the\n",
      "church\n",
      "that\n",
      "decides\n",
      "to\n",
      "provide\n",
      "transportation\n",
      "to\n",
      "a\n",
      "group\n",
      "of\n",
      "people\n",
      "that\n",
      "need\n",
      "transportation\n",
      "to\n",
      "job\n",
      "interviews\n",
      "the\n",
      "need\n",
      "transportation\n",
      "to\n",
      "their\n",
      "doctor's\n",
      "appointments\n",
      "it's\n",
      "a\n",
      "synagogue\n",
      "that\n",
      "decides\n",
      "that\n",
      "they're\n",
      "gonna\n",
      "every\n",
      "Thursday\n",
      "give\n",
      "some\n",
      "clean\n",
      "clothing\n",
      "to\n",
      "somebody\n",
      "so\n",
      "that\n",
      "they\n",
      "can\n",
      "go\n",
      "to\n",
      "that\n",
      "job\n",
      "interview\n",
      "so\n",
      "that\n",
      "they\n",
      "don't\n",
      "have\n",
      "to\n",
      "sit\n",
      "in\n",
      "filth\n",
      "it's\n",
      "the\n",
      "individual\n",
      "who\n",
      "remembers\n",
      "in\n",
      "December\n",
      "how\n",
      "cold\n",
      "it\n",
      "gets\n",
      "even\n",
      "in\n",
      "the\n",
      "desert\n",
      "man\n",
      "my\n",
      "hands\n",
      "are\n",
      "cold\n",
      "I\n",
      "bet\n",
      "you\n",
      "his\n",
      "hands\n",
      "are\n",
      "cold\n",
      "he\n",
      "doesn't\n",
      "even\n",
      "have\n",
      "a\n",
      "house\n",
      "to\n",
      "go\n",
      "to\n",
      "what\n",
      "am\n",
      "I\n",
      "gonna\n",
      "do\n",
      "I'm\n",
      "gonna\n",
      "get\n",
      "them\n",
      "gloves\n",
      "that's\n",
      "what\n",
      "I'm\n",
      "gonna\n",
      "do\n",
      "to\n",
      "help\n",
      "and\n",
      "that's\n",
      "all\n",
      "you\n",
      "need\n",
      "to\n",
      "do\n",
      "organization\n",
      "people\n",
      "coming\n",
      "together\n",
      "and\n",
      "solving\n",
      "the\n",
      "problem\n",
      "that's\n",
      "all\n",
      "it\n",
      "is\n",
      "coming\n",
      "together\n",
      "but\n",
      "what's\n",
      "being\n",
      "done\n",
      "or\n",
      "rather\n",
      "what's\n",
      "going\n",
      "on\n",
      "right\n",
      "now\n",
      "I'm\n",
      "gonna\n",
      "leave\n",
      "you\n",
      "with\n",
      "a\n",
      "story\n",
      "that\n",
      "I\n",
      "want\n",
      "to\n",
      "give\n",
      "you\n",
      "about\n",
      "we'll\n",
      "call\n",
      "him\n",
      "Jay\n",
      "I\n",
      "do\n",
      "outreach\n",
      "sometimes\n",
      "with\n",
      "the\n",
      "local\n",
      "police\n",
      "department\n",
      "Jay\n",
      "is\n",
      "19\n",
      "years\n",
      "old\n",
      "I\n",
      "found\n",
      "him\n",
      "in\n",
      "a\n",
      "ditch\n",
      "he\n",
      "was\n",
      "sleeping\n",
      "he\n",
      "wasn't\n",
      "dead\n",
      "or\n",
      "you\n",
      "know\n",
      "horrible\n",
      "but\n",
      "he's\n",
      "in\n",
      "a\n",
      "ditch\n",
      "he\n",
      "was\n",
      "kicked\n",
      "out\n",
      "of\n",
      "his\n",
      "house\n",
      "he\n",
      "was\n",
      "addicted\n",
      "to\n",
      "methamphetamines\n",
      "I\n",
      "was\n",
      "able\n",
      "with\n",
      "PD\n",
      "to\n",
      "call\n",
      "an\n",
      "individual\n",
      "that's\n",
      "in\n",
      "this\n",
      "city\n",
      "who\n",
      "runs\n",
      "a\n",
      "rehabilitation\n",
      "program\n",
      "the\n",
      "next\n",
      "day\n",
      "Jay\n",
      "was\n",
      "off\n",
      "the\n",
      "streets\n",
      "and\n",
      "he\n",
      "was\n",
      "in\n",
      "the\n",
      "rehabilitation\n",
      "program\n",
      "five\n",
      "months\n",
      "later\n",
      "today\n",
      "he\n",
      "hasn't\n",
      "touched\n",
      "any\n",
      "drugs\n",
      "he's\n",
      "no\n",
      "longer\n",
      "homeless\n",
      "he\n",
      "lives\n",
      "up\n",
      "north\n",
      "and\n",
      "he\n",
      "works\n",
      "on\n",
      "a\n",
      "farm\n",
      "that\n",
      "was\n",
      "me\n",
      "and\n",
      "one\n",
      "other\n",
      "person\n",
      "that\n",
      "did\n",
      "that\n",
      "you\n",
      "can\n",
      "do\n",
      "it\n",
      "too\n",
      "we\n",
      "can\n",
      "solve\n",
      "this\n",
      "crisis\n",
      "we\n",
      "can\n",
      "solve\n",
      "this\n",
      "problem\n",
      "we\n",
      "just\n",
      "have\n",
      "to\n",
      "come\n",
      "together\n",
      "[Applause]\n",
      "[Music]\n",
      "you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 334\n",
    "prt('H',Y_train[i])\n",
    "prt('D',X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\n",
      "Evolve\n",
      "Through\n",
      "Adversity\n",
      "Curtis\n",
      "Voelker\n",
      "TEDxHarrisburg\n",
      "\n",
      "D:\n",
      "[Applause]\n",
      "when\n",
      "you\n",
      "were\n",
      "14\n",
      "years\n",
      "old\n",
      "how\n",
      "did\n",
      "you\n",
      "evolve\n",
      "wait\n",
      "a\n",
      "minute\n",
      "we're\n",
      "probably\n",
      "all\n",
      "having\n",
      "very\n",
      "different\n",
      "thoughts\n",
      "right\n",
      "now\n",
      "in\n",
      "terms\n",
      "of\n",
      "evolving\n",
      "at\n",
      "the\n",
      "age\n",
      "of\n",
      "14\n",
      "what\n",
      "I'm\n",
      "asking\n",
      "you\n",
      "is\n",
      "what\n",
      "were\n",
      "the\n",
      "major\n",
      "choices\n",
      "that\n",
      "you\n",
      "made\n",
      "was\n",
      "there\n",
      "anything\n",
      "that\n",
      "really\n",
      "changed\n",
      "your\n",
      "life\n",
      "or\n",
      "your\n",
      "major\n",
      "choices\n",
      "trying\n",
      "out\n",
      "for\n",
      "the\n",
      "high\n",
      "school\n",
      "sports\n",
      "team\n",
      "or\n",
      "was\n",
      "it\n",
      "starting\n",
      "your\n",
      "first\n",
      "part-time\n",
      "job\n",
      "for\n",
      "some\n",
      "extra\n",
      "spending\n",
      "money\n",
      "or\n",
      "was\n",
      "it\n",
      "finally\n",
      "asking\n",
      "out\n",
      "that\n",
      "special\n",
      "person\n",
      "in\n",
      "your\n",
      "class\n",
      "for\n",
      "myself\n",
      "at\n",
      "the\n",
      "age\n",
      "of\n",
      "14\n",
      "my\n",
      "life-changing\n",
      "moment\n",
      "happened\n",
      "on\n",
      "a\n",
      "simple\n",
      "Saturday\n",
      "morning\n",
      "where\n",
      "I\n",
      "saw\n",
      "my\n",
      "parents\n",
      "arrested\n",
      "in\n",
      "front\n",
      "of\n",
      "me\n",
      "and\n",
      "when\n",
      "the\n",
      "dust\n",
      "settled\n",
      "I\n",
      "was\n",
      "moved\n",
      "in\n",
      "with\n",
      "my\n",
      "aunt\n",
      "my\n",
      "uncle\n",
      "through\n",
      "a\n",
      "form\n",
      "of\n",
      "kinship\n",
      "care\n",
      "with\n",
      "our\n",
      "local\n",
      "children\n",
      "and\n",
      "youth\n",
      "services\n",
      "and\n",
      "in\n",
      "this\n",
      "new\n",
      "setting\n",
      "this\n",
      "new\n",
      "town\n",
      "this\n",
      "new\n",
      "high\n",
      "school\n",
      "this\n",
      "new\n",
      "family\n",
      "this\n",
      "new\n",
      "lifestyle\n",
      "I\n",
      "was\n",
      "able\n",
      "to\n",
      "experience\n",
      "many\n",
      "new\n",
      "things\n",
      "I\n",
      "was\n",
      "able\n",
      "to\n",
      "join\n",
      "school\n",
      "clubs\n",
      "activities\n",
      "and\n",
      "sports\n",
      "and\n",
      "through\n",
      "transitioning\n",
      "through\n",
      "my\n",
      "adversity\n",
      "local\n",
      "children\n",
      "need\n",
      "services\n",
      "asked\n",
      "me\n",
      "to\n",
      "begin\n",
      "sharing\n",
      "my\n",
      "story\n",
      "with\n",
      "other\n",
      "foster\n",
      "care\n",
      "children\n",
      "in\n",
      "my\n",
      "area\n",
      "and\n",
      "it\n",
      "was\n",
      "through\n",
      "sharing\n",
      "my\n",
      "story\n",
      "that\n",
      "I\n",
      "found\n",
      "my\n",
      "true\n",
      "passion\n",
      "for\n",
      "public\n",
      "speaking\n",
      "now\n",
      "when\n",
      "everything\n",
      "happened\n",
      "in\n",
      "my\n",
      "life\n",
      "I\n",
      "had\n",
      "some\n",
      "great\n",
      "mentors\n",
      "but\n",
      "I\n",
      "didn't\n",
      "necessarily\n",
      "turn\n",
      "to\n",
      "anyone\n",
      "in\n",
      "the\n",
      "real\n",
      "world\n",
      "I\n",
      "turned\n",
      "to\n",
      "the\n",
      "fictional\n",
      "character\n",
      "of\n",
      "Superman\n",
      "you're\n",
      "probably\n",
      "expecting\n",
      "the\n",
      "shirt\n",
      "to\n",
      "rip\n",
      "off\n",
      "but\n",
      "I\n",
      "didn't\n",
      "turn\n",
      "to\n",
      "Superman\n",
      "because\n",
      "he\n",
      "was\n",
      "faster\n",
      "than\n",
      "a\n",
      "speeding\n",
      "bullet\n",
      "more\n",
      "powerful\n",
      "than\n",
      "a\n",
      "locomotive\n",
      "or\n",
      "he\n",
      "can\n",
      "leap\n",
      "tall\n",
      "buildings\n",
      "in\n",
      "a\n",
      "single\n",
      "bound\n",
      "I\n",
      "turned\n",
      "to\n",
      "Superman\n",
      "because\n",
      "of\n",
      "what\n",
      "he\n",
      "stood\n",
      "for\n",
      "because\n",
      "he\n",
      "always\n",
      "strived\n",
      "to\n",
      "see\n",
      "the\n",
      "good\n",
      "in\n",
      "people\n",
      "and\n",
      "because\n",
      "he\n",
      "never\n",
      "gave\n",
      "up\n",
      "and\n",
      "I\n",
      "realized\n",
      "that\n",
      "through\n",
      "sharing\n",
      "my\n",
      "story\n",
      "through\n",
      "my\n",
      "speaking\n",
      "that\n",
      "just\n",
      "as\n",
      "I\n",
      "saw\n",
      "Superman\n",
      "as\n",
      "a\n",
      "symbol\n",
      "of\n",
      "hope\n",
      "and\n",
      "motivation\n",
      "for\n",
      "myself\n",
      "that\n",
      "I\n",
      "could\n",
      "be\n",
      "a\n",
      "symbol\n",
      "of\n",
      "hope\n",
      "and\n",
      "motivation\n",
      "for\n",
      "others\n",
      "in\n",
      "fact\n",
      "we\n",
      "all\n",
      "can\n",
      "because\n",
      "whether\n",
      "you\n",
      "know\n",
      "what\n",
      "or\n",
      "not\n",
      "we\n",
      "all\n",
      "have\n",
      "a\n",
      "superpower\n",
      "deep\n",
      "inside\n",
      "of\n",
      "us\n",
      "a\n",
      "superpower\n",
      "I\n",
      "like\n",
      "to\n",
      "call\n",
      "the\n",
      "storm\n",
      "of\n",
      "success\n",
      "ladies\n",
      "and\n",
      "gentlemen\n",
      "right\n",
      "here\n",
      "right\n",
      "now\n",
      "you\n",
      "are\n",
      "all\n",
      "going\n",
      "to\n",
      "see\n",
      "hear\n",
      "feel\n",
      "and\n",
      "experience\n",
      "the\n",
      "power\n",
      "of\n",
      "the\n",
      "storm\n",
      "of\n",
      "success\n",
      "so\n",
      "we're\n",
      "gonna\n",
      "start\n",
      "right\n",
      "here\n",
      "and\n",
      "when\n",
      "I\n",
      "gesture\n",
      "from\n",
      "side\n",
      "to\n",
      "side\n",
      "of\n",
      "the\n",
      "room\n",
      "from\n",
      "the\n",
      "front\n",
      "all\n",
      "the\n",
      "way\n",
      "to\n",
      "the\n",
      "very\n",
      "back\n",
      "I\n",
      "need\n",
      "you\n",
      "to\n",
      "go\n",
      "along\n",
      "with\n",
      "the\n",
      "action\n",
      "that\n",
      "is\n",
      "part\n",
      "of\n",
      "our\n",
      "storm\n",
      "of\n",
      "success\n",
      "so\n",
      "from\n",
      "right\n",
      "here\n",
      "all\n",
      "the\n",
      "way\n",
      "to\n",
      "the\n",
      "back\n",
      "if\n",
      "you\n",
      "would\n",
      "simply\n",
      "start\n",
      "snapping\n",
      "your\n",
      "fingers\n",
      "for\n",
      "me\n",
      "and\n",
      "our\n",
      "storms\n",
      "gonna\n",
      "start\n",
      "and\n",
      "if\n",
      "you\n",
      "all\n",
      "start\n",
      "stamping\n",
      "your\n",
      "fingers\n",
      "for\n",
      "me\n",
      "and\n",
      "the\n",
      "rest\n",
      "of\n",
      "the\n",
      "room\n",
      "and\n",
      "just\n",
      "listen\n",
      "for\n",
      "me\n",
      "as\n",
      "you\n",
      "snap\n",
      "our\n",
      "little\n",
      "drops\n",
      "of\n",
      "rain\n",
      "our\n",
      "storm\n",
      "of\n",
      "success\n",
      "is\n",
      "starting\n",
      "and\n",
      "this\n",
      "is\n",
      "often\n",
      "times\n",
      "how\n",
      "we\n",
      "may\n",
      "feel\n",
      "when\n",
      "everything\n",
      "is\n",
      "going\n",
      "wrong\n",
      "in\n",
      "our\n",
      "lives\n",
      "small\n",
      "weak\n",
      "and\n",
      "significant\n",
      "drops\n",
      "of\n",
      "rain\n",
      "but\n",
      "then\n",
      "we\n",
      "start\n",
      "to\n",
      "meet\n",
      "others\n",
      "keep\n",
      "snapping\n",
      "for\n",
      "me\n",
      "our\n",
      "storm\n",
      "is\n",
      "not\n",
      "over\n",
      "yet\n",
      "we\n",
      "start\n",
      "to\n",
      "meet\n",
      "others\n",
      "to\n",
      "get\n",
      "involved\n",
      "with\n",
      "our\n",
      "community\n",
      "and\n",
      "our\n",
      "friends\n",
      "and\n",
      "if\n",
      "you\n",
      "wouldn't\n",
      "start\n",
      "patting\n",
      "your\n",
      "thighs\n",
      "for\n",
      "me\n",
      "not\n",
      "too\n",
      "hard\n",
      "not\n",
      "too\n",
      "soft\n",
      "there\n",
      "we\n",
      "go\n",
      "and\n",
      "if\n",
      "we'd\n",
      "start\n",
      "patting\n",
      "our\n",
      "thighs\n",
      "for\n",
      "a\n",
      "patting\n",
      "our\n",
      "thighs\n",
      "patting\n",
      "our\n",
      "thighs\n",
      "rest\n",
      "the\n",
      "room\n",
      "the\n",
      "storm\n",
      "is\n",
      "getting\n",
      "a\n",
      "little\n",
      "bit\n",
      "louder\n",
      "a\n",
      "little\n",
      "bit\n",
      "more\n",
      "intense\n",
      "we're\n",
      "starting\n",
      "to\n",
      "give\n",
      "back\n",
      "and\n",
      "make\n",
      "an\n",
      "impact\n",
      "in\n",
      "our\n",
      "school\n",
      "maybe\n",
      "we're\n",
      "giving\n",
      "donations\n",
      "to\n",
      "a\n",
      "local\n",
      "food\n",
      "shelter\n",
      "we're\n",
      "giving\n",
      "our\n",
      "time\n",
      "to\n",
      "local\n",
      "charities\n",
      "and\n",
      "then\n",
      "everything\n",
      "just\n",
      "simply\n",
      "comes\n",
      "together\n",
      "if\n",
      "you\n",
      "would\n",
      "all\n",
      "start\n",
      "stop\n",
      "your\n",
      "feet\n",
      "for\n",
      "me\n",
      "and\n",
      "stomping\n",
      "our\n",
      "feet\n",
      "and\n",
      "stopping\n",
      "our\n",
      "feet\n",
      "and\n",
      "stomping\n",
      "our\n",
      "feet\n",
      "the\n",
      "storm\n",
      "is\n",
      "picking\n",
      "up\n",
      "we're\n",
      "feeling\n",
      "that\n",
      "power\n",
      "we're\n",
      "healing\n",
      "that\n",
      "hearing\n",
      "that\n",
      "power\n",
      "and\n",
      "the\n",
      "storm\n",
      "is\n",
      "almost\n",
      "at\n",
      "its\n",
      "peak\n",
      "if\n",
      "you\n",
      "would\n",
      "stop\n",
      "your\n",
      "feet\n",
      "and\n",
      "pat\n",
      "your\n",
      "thighs\n",
      "stomp\n",
      "your\n",
      "feet\n",
      "and\n",
      "Pat\n",
      "your\n",
      "thighs\n",
      "stomp\n",
      "your\n",
      "feet\n",
      "and\n",
      "Pat\n",
      "your\n",
      "thighs\n",
      "everyone\n",
      "give\n",
      "me\n",
      "everything\n",
      "you\n",
      "got\n",
      "and\n",
      "simply\n",
      "listen\n",
      "the\n",
      "Thunder\n",
      "the\n",
      "lightning\n",
      "we're\n",
      "making\n",
      "that\n",
      "impact\n",
      "we\n",
      "see\n",
      "the\n",
      "power\n",
      "we\n",
      "have\n",
      "and\n",
      "if\n",
      "you\n",
      "would\n",
      "all\n",
      "just\n",
      "simply\n",
      "snap\n",
      "your\n",
      "fingers\n",
      "and\n",
      "just\n",
      "simply\n",
      "snap\n",
      "our\n",
      "fingers\n",
      "and\n",
      "just\n",
      "simply\n",
      "snap\n",
      "our\n",
      "fingers\n",
      "there's\n",
      "storm\n",
      "is\n",
      "calming\n",
      "down\n",
      "and\n",
      "if\n",
      "we\n",
      "just\n",
      "slowly\n",
      "stop\n",
      "and\n",
      "slowly\n",
      "stop\n",
      "slowly\n",
      "stop\n",
      "there\n",
      "we\n",
      "go\n",
      "their\n",
      "storm\n",
      "has\n",
      "come\n",
      "and\n",
      "gone\n",
      "we\n",
      "all\n",
      "saw\n",
      "heard\n",
      "felt\n",
      "and\n",
      "experienced\n",
      "the\n",
      "super\n",
      "power\n",
      "we\n",
      "have\n",
      "inside\n",
      "of\n",
      "us\n",
      "now\n",
      "I\n",
      "encourage\n",
      "you\n",
      "all\n",
      "to\n",
      "flood\n",
      "your\n",
      "communities\n",
      "flood\n",
      "this\n",
      "nation\n",
      "and\n",
      "flood\n",
      "this\n",
      "world\n",
      "with\n",
      "our\n",
      "storm\n",
      "a\n",
      "success\n",
      "as\n",
      "we\n",
      "help\n",
      "each\n",
      "other\n",
      "and\n",
      "ourselves\n",
      "evolve\n",
      "through\n",
      "adversity\n",
      "thank\n",
      "you\n",
      "[Applause]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 334\n",
    "prt('H',Y_test[i])\n",
    "prt('D',X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Merge, SpatialDropout1D\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with standard stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model with sequential\n",
    "model = Sequential()\n",
    "\n",
    "#add an embedding layer\n",
    "model.add(Embedding(vocab_size,embedding_size,\n",
    "                   input_length = maxlen, \n",
    "                   activity_regularizer = regularizer, weights = [embedding],\n",
    "                   mask_zero = True, name = 'embedding_1'))\n",
    "\n",
    "#add the dropout layer (replaces the dropout paramater in embedding layer)\n",
    "#model.add(SpatialDropout1D(rate = p_emb,name='dropout_0'))\n",
    "\n",
    "\n",
    "#add #rnn_layers (start with 3) of LSTMs\n",
    "for i in range(rnn_layers):\n",
    "    lstm = LSTM(rnn_size, return_sequences=True, #batch_norm = batch_norm,\n",
    "                kernel_regularizer = regularizer, recurrent_regularizer = regularizer,\n",
    "                bias_regularizer = regularizer, dropout = p_W, recurrent_dropout = p_U,\n",
    "                name = 'lstm_%d'%(i+1)\n",
    "               )\n",
    "    \n",
    "    model.add(lstm)\n",
    "    #add a dropout layer for each LSTM\n",
    "    model.add(Dropout(rate = p_dense, name = 'dropout_%d'%(i+1)))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "def simple_context(X, mask, n = activation_rnn_size, \n",
    "                   maxlend = maxlend, maxlenh = maxlenh):\n",
    "    desc, head = X[:,:maxlend,:], X[:,maxlend:,:]\n",
    "    head_activations, head_words = head[:,:,:n],head[:,:,n:]\n",
    "    desc_activations, desc_words = desc[:,:,:n],desc[:,:,n:]\n",
    "    \n",
    "    #activation for every head word and desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2,2))\n",
    "    \n",
    "    #don't use description words that are masked out\n",
    "    activation_energies = activation_energies + -1e20*K.expand_dims(1.-K.cast(mask[:,:maxlend],dtype='float32'),1)\n",
    "    \n",
    "    #for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights, (-1,maxlenh,maxlend))\n",
    "    \n",
    "    #for every head word compute weighted average of desc words\n",
    "    desc_avg_word = K.batch_dot(activation_weights,desc_words,axes=(2,1))\n",
    "    return K.concatenate((desc_avg_word,head_words))\n",
    "\n",
    "\n",
    "class SimpleContext(Lambda):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(SimpleContext, self).__init__(simple_context,**kwargs)\n",
    "        self.supports_masking = True\n",
    "    \n",
    "    def compute_mask(self,input, input_mask=None):\n",
    "        return input_mask[:,maxlend:]\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        nb_samples = input_shape[0]\n",
    "        n = 2*(rnn_size - activation_rnn_size)\n",
    "        return (nb_samples,maxlenh,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if activation_rnn_size:\n",
    "    model.add(SimpleContext(name='simplecontext_1'))\n",
    "    \n",
    "model.add(TimeDistributed(Dense(vocab_size,\n",
    "                               kernel_regularizer = regularizer, bias_regularizer = regularizer,\n",
    "                               name = 'timedistributed_1')))\n",
    "model.add(Activation('softmax',name='activation_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "new Audio(\"http://www.soundjay.com/button/beep-09.wav\").play()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "new Audio(\"http://www.soundjay.com/button/beep-09.wav\").play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr,np.float32(LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_shape(x):\n",
    "    return 'x'.join(map(str,x.shape))\n",
    "    \n",
    "def inspect_model(model):\n",
    "    for i,l in enumerate(model.layers):\n",
    "        print (i, 'cls=%s name=%s'%(type(l).__name__, l.name))\n",
    "        weights = l.get_weights()\n",
    "        for weight in weights:\n",
    "            print (str_shape(weight),)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, <keras.layers.embeddings.Embedding object at 0x11de8e470>)\n",
      "(1, <keras.layers.recurrent.LSTM object at 0x11e1ef5c0>)\n",
      "(2, <keras.layers.core.Dropout object at 0x11e1ef780>)\n",
      "(3, <keras.layers.recurrent.LSTM object at 0x113696eb8>)\n",
      "(4, <keras.layers.core.Dropout object at 0x10aed9b70>)\n",
      "(5, <keras.layers.recurrent.LSTM object at 0x10ecdcef0>)\n",
      "(6, <keras.layers.core.Dropout object at 0x10ecdc6d8>)\n",
      "(7, <__main__.SimpleContext object at 0x10ece4438>)\n",
      "(8, <keras.layers.wrappers.TimeDistributed object at 0x109b58710>)\n",
      "(9, <keras.layers.core.Activation object at 0x10ecec550>)\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(model.layers):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls=Embedding name=embedding_1\n",
      "40000x100\n",
      "\n",
      "1 cls=LSTM name=lstm_1\n",
      "100x2048\n",
      "512x2048\n",
      "2048\n",
      "\n",
      "2 cls=Dropout name=dropout_1\n",
      "\n",
      "3 cls=LSTM name=lstm_2\n",
      "512x2048\n",
      "512x2048\n",
      "2048\n",
      "\n",
      "4 cls=Dropout name=dropout_2\n",
      "\n",
      "5 cls=LSTM name=lstm_3\n",
      "512x2048\n",
      "512x2048\n",
      "2048\n",
      "\n",
      "6 cls=Dropout name=dropout_3\n",
      "\n",
      "7 cls=SimpleContext name=simplecontext_1\n",
      "\n",
      "8 cls=TimeDistributed name=time_distributed_1\n",
      "944x40000\n",
      "40000\n",
      "\n",
      "9 cls=Activation name=activation_1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FN1 and os.path.exists('data/%s.hdf5'%FN1):\n",
    "    model.load_weights('data/%s.hdf5'%FN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpadd(x,maxlend=maxlend,eos=eos):\n",
    "    \"\"\"left pad a description to maxlend and then add eos.\n",
    "    The eos is the input to predicting the first word in the headline\"\"\"\n",
    "    assert maxlend >= 0\n",
    "    if maxlend == 0:\n",
    "        return [eos]\n",
    "    n = len(x)\n",
    "    if n > maxlend:\n",
    "        x = x[-maxlend:]\n",
    "        n = maxlend\n",
    "    return [empty]*(maxlend-n) + x + [eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lpadd([3]*26)]\n",
    "data = sequence.pad_sequences(samples,maxlen=maxlen, value=empty,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(data[:,maxlend]==eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 125), [101])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,list(map(len,samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 326ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 25, 40000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(data,verbose=1,batch_size=1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamsearch(predict, start=[empty]*maxlend + [eos],\n",
    "               k=1, maxsample=maxlen, use_unk=True, empty=empty, eos=eos, temperature=1.0):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    def sample(energy, n, temperature=temperature):\n",
    "        \"\"\"sample at most n elements according to their energy\"\"\"\n",
    "        n = min(n,len(energy))\n",
    "        prb = np.exp(-np.array(energy) / temperature )\n",
    "        res = []\n",
    "        for i in range(n):\n",
    "            z = np.sum(prb)\n",
    "            r = np.argmax(np.random.multinomial(1, prb/z, 1))\n",
    "            res.append(r)\n",
    "            prb[r] = 0. # make sure we select each element only once\n",
    "        return res\n",
    "\n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [list(start)]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        \n",
    "        a = np.array(live_scores)[:,None]\n",
    "        b = np.log(np.array((probs)))\n",
    "        \n",
    "        cand_scores = a-b\n",
    "\n",
    "        cand_scores[:,empty] = 1e20\n",
    "        if not use_unk:\n",
    "            for i in range(nb_unknown_words):\n",
    "                cand_scores[:,vocab_size - 1 - i] = 1e20\n",
    "        live_scores = list(cand_scores.flatten())\n",
    "        \n",
    "\n",
    "        # find the best (lowest) scores we have from all possible dead samples and\n",
    "        # all live samples and all possible new words added\n",
    "        scores = dead_scores + live_scores\n",
    "        ranks = sample(scores, k)\n",
    "        n = len(dead_scores)\n",
    "        ranks_dead = [r for r in ranks if r < n]\n",
    "        ranks_live = [r - n for r in ranks if r >= n]\n",
    "        \n",
    "        dead_scores = [dead_scores[r] for r in ranks_dead]\n",
    "        dead_samples = [dead_samples[r] for r in ranks_dead]\n",
    "        \n",
    "        live_scores = [live_scores[r] for r in ranks_live]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        \n",
    "        voc_size = probs.shape[2]\n",
    "        print('voc_size: ',voc_size)\n",
    "        print('live_samples: ',live_samples)\n",
    "        print('ranks_live: ', ranks_live)\n",
    "        \n",
    "        a = [live_samples[0][r//voc_size] for r in ranks_live]\n",
    "        b = [r%voc_size for r in ranks_live]\n",
    "        live_samples = [a+b]\n",
    "        print('live_samples:' , live_samples)\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        # even if len(live_samples) == maxsample we dont want it dead because we want one\n",
    "        # last prediction out of it to reach a headline of maxlenh\n",
    "        zombie = [s[-1] == eos or len(s) > maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    return dead_samples + live_samples, dead_scores + live_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_rnn_predict(samples,empty=empty,model=model,maxlen=maxlen):\n",
    "    \"\"\"for each sample, calculate probablity for every possible label\n",
    "    need to supply RNN model and maxlen\"\"\"\n",
    "    sample_lengths = list(map(len,samples))\n",
    "    print(sample_)\n",
    "    assert all(l > maxlend for l in sample_lengths)\n",
    "    assert all(l[maxlend] == eos for l in samples)\n",
    "    # pad from right so the first maxlend will be description followed by headline\n",
    "    data = sequence.pad_sequences(samples,maxlen=maxlen,value=empty,padding='post',truncating='post')\n",
    "    probs = model.predict(data,verbose=0,batch_size=batch_size)\n",
    "    return probs\n",
    "    return np.array(list([prob[sample_length-maxlend-1]] for prob,sample_length in zip(probs,sample_lengths)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_fold(xs):\n",
    "    '''\n",
    "    Convert a list of word of indices that may contain words outside vocab size\n",
    "    to words inside.  If a word is outside, first try glove_idx2idx to find a similar word inside.\n",
    "    If none exist then replace all occurences of the same unknown word with <0>,<1>,etc.\n",
    "    '''\n",
    "    xs = [x if x < oov0 else glove_idx2idx.get(x,x) for x in xs]\n",
    "    #the most popular word is <0> and so on.\n",
    "    outside = sorted([x for x in xs if x >= oov0])\n",
    "    # if there are more than nb_unknown_words oov words then put them all in nb_unknown_words - 1\n",
    "    outside = dict((x,vocab_size-1-min(i, nb_unknown_words-1)) for i, x in enumerate(outside))    \n",
    "    xs = [outside.get(x,x) for x in xs]\n",
    "    return xs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_unfold(desc,xs):\n",
    "    #assume desc is the unfolded version of the start of xs\n",
    "    unfold = {}\n",
    "    for i, unfold_idx in enumerate(desc):\n",
    "        fold_idx = xs[i]\n",
    "        if fold_idx >= oov0:\n",
    "            unfold[fold_idx] = unfold_idx\n",
    "    return [unfold.get(x,x) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import Levenshtein\n",
    "\n",
    "def gensamples(skips=2, k=10, batch_size=batch_size, short=True, temperature=1., use_unk=True):\n",
    "    i = random.randint(0,len(X_test)-1)\n",
    "    print( 'HEAD:',' '.join(idx2word[w] for w in Y_test[i][:maxlenh]))\n",
    "    print( 'DESC:',' '.join(idx2word[w] for w in X_test[i][:maxlend]))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print( 'HEADS:')\n",
    "    x = X_test[i]\n",
    "    samples = []\n",
    "    if maxlend == 0:\n",
    "        skips = [0]\n",
    "    else:\n",
    "        skips = range(min(maxlend,len(x)), max(maxlend,len(x)), abs(maxlend - len(x)) // skips + 1)\n",
    "    for s in skips:\n",
    "        start = lpadd(x[:s])\n",
    "        fold_start = vocab_fold(start)\n",
    "        sample, score = beamsearch(predict=keras_rnn_predict, start=fold_start, k=k, temperature=temperature, use_unk=use_unk)\n",
    "        assert all(s[maxlend] == eos for s in sample)\n",
    "        samples += [(s,start,scr) for s,scr in zip(sample,score)]\n",
    "\n",
    "    samples.sort(key=lambda x: x[-1])\n",
    "    codes = []\n",
    "    for sample, start, score in samples:\n",
    "        code = ''\n",
    "        words = []\n",
    "        sample = vocab_unfold(start, sample)[len(start):]\n",
    "        for w in sample:\n",
    "            if w == eos:\n",
    "                break\n",
    "            words.append(idx2word[w])\n",
    "            code += chr(w//(256*256)) + chr((w//256)%256) + chr(w%256)\n",
    "        if short:\n",
    "            distance = min([100] + [-Levenshtein.jaro(code,c) for c in codes])\n",
    "            if distance > -0.6:\n",
    "                print (score, ' '.join(words))\n",
    "        #         print '%s (%.2f) %f'%(' '.join(words), score, distance)\n",
    "        else:\n",
    "                print (score, ' '.join(words))\n",
    "        codes.append(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD: How computers learn to recognize objects instantly Joseph Redmon\n",
      "DESC: Ten years ago, computer vision researchers thought that getting a computer to tell the difference between a cat and a dog would be almost impossible, even with the significant advance in the state of artificial intelligence. Now we can do it at a level greater than 99 percent accuracy. This is called image classification  give it an image, put a label to that image  and computers know thousands of other categories as well. I'm a graduate student at the University of Washington, and I work on a project called Darknet, which is a neural network framework for training\n",
      "HEADS:\n",
      "voc_size:  40000\n",
      "live_samples:  [[6111, 100, 650, 524, 922, 1725, 194, 7, 270, 6, 524, 4, 154, 2, 625, 203, 6, 2672, 3, 6, 1608, 63, 25, 365, 10589, 104, 20, 2, 1570, 3778, 9, 2, 587, 5, 1449, 7029, 306, 12, 26, 34, 13, 32, 6, 594, 1255, 101, 4618, 228, 15793, 128, 11, 153, 719, 13580, 52, 175, 13, 42, 4504, 158, 6, 3405, 4, 7, 719, 52, 3, 1316, 54, 568, 5, 90, 6047, 30, 1020, 66, 6, 2103, 845, 32, 2, 1064, 5, 6892, 3, 8, 125, 21, 6, 510, 153, 39999, 72, 11, 6, 2918, 1143, 3435, 16, 1008, 1]]\n",
      "ranks_live:  [653481, 882988, 750173, 591453, 47565, 226327, 234318, 864263, 511779, 930689]\n",
      "live_samples: [[6, 25, 3, 625, 100, 1725, 1725, 63, 154, 365, 13481, 2988, 30173, 31453, 7565, 26327, 34318, 24263, 31779, 10689]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-c7181e116100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgensamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-4631acc68ce2>\u001b[0m in \u001b[0;36mgensamples\u001b[0;34m(skips, k, batch_size, short, temperature, use_unk)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlpadd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfold_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeamsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras_rnn_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_unk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaxlend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meos\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-7637b13084f3>\u001b[0m in \u001b[0;36mbeamsearch\u001b[0;34m(predict, start, k, maxsample, use_unk, empty, eos, temperature)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlive_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# for every possible live sample calc prob for every possible label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlive_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# total score for every sample is sum of -log of word prb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-f6cae2d27a87>\u001b[0m in \u001b[0;36mkeras_rnn_predict\u001b[0;34m(samples, empty, model, maxlen)\u001b[0m\n\u001b[1;32m      3\u001b[0m     need to supply RNN model and maxlen\"\"\"\n\u001b[1;32m      4\u001b[0m     \u001b[0msample_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxlend\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaxlend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meos\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# pad from right so the first maxlend will be description followed by headline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gensamples(skips=2,batch_size=batch_size,k=10,temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_headline(x, nflips=None, model=None, debug=False):\n",
    "    \"\"\"given a vectorized input (after `pad_sequences`) flip some of the words in the second half (headline)\n",
    "    with words predicted by the model\n",
    "    \"\"\"\n",
    "    if nflips is None or model is None or nflips <= 0:\n",
    "        return x\n",
    "    \n",
    "    batch_size = len(x)\n",
    "    assert np.all(x[:,maxlend] == eos)\n",
    "    probs = model.predict(x, verbose=0, batch_size=batch_size)\n",
    "    x_out = x.copy()\n",
    "    for b in range(batch_size):\n",
    "        # pick locations we want to flip\n",
    "        # 0...maxlend-1 are descriptions and should be fixed\n",
    "        # maxlend is eos and should be fixed\n",
    "        flips = sorted(random.sample(range(maxlend+1,maxlen), nflips))\n",
    "        if debug and b < debug:\n",
    "            print (b,)\n",
    "        for input_idx in flips:\n",
    "            if x[b,input_idx] == empty or x[b,input_idx] == eos:\n",
    "                continue\n",
    "            # convert from input location to label location\n",
    "            # the output at maxlend (when input is eos) is feed as input at maxlend+1\n",
    "            label_idx = input_idx - (maxlend+1)\n",
    "            prob = probs[b, label_idx]\n",
    "            w = prob.argmax()\n",
    "            if w == empty:  # replace accidental empty with oov\n",
    "                w = oov0\n",
    "            if debug and b < debug:\n",
    "                print ('%s => %s'%(idx2word[x_out[b,input_idx]],idx2word[w]),)\n",
    "            x_out[b,input_idx] = w\n",
    "        if debug and b < debug:\n",
    "            print()\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def conv_seq_labels(xds, xhs, nflips=None, model=None, debug=False):\n",
    "        \"\"\"description and hedlines are converted to padded input vectors. headlines are one-hot to label\"\"\"\n",
    "        batch_size = len(xhs)\n",
    "        assert len(xds) == batch_size\n",
    "        x = [vocab_fold(lpadd(xd)+xh) for xd,xh in zip(xds,xhs)]  # the input does not have 2nd eos\n",
    "        x = sequence.pad_sequences(x, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "        x = flip_headline(x, nflips=nflips, model=model, debug=debug)\n",
    "\n",
    "        y = np.zeros((batch_size, maxlenh, vocab_size))\n",
    "        for i, xh in enumerate(xhs):\n",
    "            xh = vocab_fold(xh) + [eos] + [empty]*maxlenh  # output does have a eos at end\n",
    "            xh = xh[:maxlenh]\n",
    "            y[i,:,:] = np_utils.to_categorical(xh, vocab_size)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(Xd, Xh, batch_size=batch_size, nb_batches=None, nflips=None, model=None, debug=False, seed=seed):\n",
    "    \"\"\"yield batches. for training use nb_batches=None\n",
    "    for validation generate deterministic results repeating every nb_batches\n",
    "    \n",
    "    while training it is good idea to flip once in a while the values of the headlines from the\n",
    "    value taken from Xh to value generated by the model.\n",
    "    \"\"\"\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        xds = []\n",
    "        xhs = []\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        for b in range(batch_size):\n",
    "            t = random.randint(0,len(Xd)-1)\n",
    "\n",
    "            xd = Xd[t]\n",
    "            s = random.randint(min(maxlend,len(xd)), max(maxlend,len(xd)))\n",
    "            xds.append(xd[:s])\n",
    "            \n",
    "            xh = Xh[t]\n",
    "            s = random.randint(min(maxlenh,len(xh)), max(maxlenh,len(xh)))\n",
    "            xhs.append(xh[:s])\n",
    "\n",
    "        # undo the seeding before we yield inorder not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(xds, xhs, nflips=nflips, model=model, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 125), (64, 25, 40000), 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = next(gen(X_train, Y_train, batch_size=batch_size))\n",
    "r[0].shape, r[1].shape, len(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gen(gen, n=5):\n",
    "    Xtr,Ytr = next(gen)\n",
    "    for i in range(n):\n",
    "        assert Xtr[i,maxlend] == eos\n",
    "        x = Xtr[i,:maxlend]\n",
    "        y = Xtr[i,maxlend:]\n",
    "        yy = Ytr[i,:]\n",
    "        yy = np.where(yy)[1]\n",
    "        prt('L',yy)\n",
    "        prt('H',y)\n",
    "        if maxlend:\n",
    "            prt('D',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "seeking\n",
      "the\n",
      "Truth\n",
      "in\n",
      "Human\n",
      "trafficking\n",
      "and\n",
      "other\n",
      "things\n",
      "too\n",
      "John\n",
      "<0>\n",
      "TEDxCarsonCity\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "seeking\n",
      "the\n",
      "Truth\n",
      "in\n",
      "Human\n",
      "trafficking\n",
      "and\n",
      "other\n",
      "things\n",
      "too\n",
      "John\n",
      "<0>\n",
      "TEDxCarsonCity\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "right\n",
      "or\n",
      "wrong\n",
      "good\n",
      "or\n",
      "bad\n",
      "just\n",
      "or\n",
      "unjust\n",
      "true\n",
      "or\n",
      "false\n",
      "they\n",
      "they\n",
      "try\n",
      "to\n",
      "paint\n",
      "us\n",
      "into\n",
      "corners\n",
      "where\n",
      "seemingly\n",
      "our\n",
      "only\n",
      "choices\n",
      "are\n",
      "to\n",
      "either\n",
      "completely\n",
      "agree\n",
      "or\n",
      "completely\n",
      "disagree\n",
      "they\n",
      "neglect\n",
      "to\n",
      "tell\n",
      "us\n",
      "about\n",
      "the\n",
      "good\n",
      "work\n",
      "that\n",
      "happens\n",
      "when\n",
      "people\n",
      "meet\n",
      "in\n",
      "the\n",
      "middle\n",
      "and\n",
      "share\n",
      "their\n",
      "passion\n",
      "and\n",
      "their\n",
      "knowledge\n",
      "and\n",
      "we're\n",
      "real\n",
      "effective\n",
      "change\n",
      "can\n",
      "happen\n",
      "so\n",
      "I\n",
      "would\n",
      "urge\n",
      "you\n",
      "if\n",
      "any\n",
      "of\n",
      "these\n",
      "important\n",
      "issues\n",
      "that\n",
      "we're\n",
      "talking\n",
      "about\n",
      "many\n",
      "of\n",
      "which\n",
      "we\n",
      "are\n",
      "talking\n",
      "about\n",
      "today\n",
      "if\n",
      "they\n",
      "resonate\n",
      "within\n",
      "you\n",
      "embrace\n",
      "their\n",
      "complexities\n",
      "study\n",
      "them\n",
      "really\n",
      "learn\n",
      "talk\n",
      "\n",
      "L:\n",
      "The\n",
      "science\n",
      "of\n",
      "snowflakes\n",
      "<0>\n",
      "<1>\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "The\n",
      "science\n",
      "of\n",
      "snowflakes\n",
      "<1>\n",
      "<2>\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "are\n",
      "similar,\n",
      "a\n",
      "symmetric\n",
      "snowflake\n",
      "can\n",
      "grow.\n",
      "Weather\n",
      "conditions\n",
      "affect\n",
      "snow\n",
      "on\n",
      "the\n",
      "ground,\n",
      "as\n",
      "well.\n",
      "warmer\n",
      "ground\n",
      "temperatures\n",
      "produce\n",
      "a\n",
      "colder\n",
      "snow\n",
      "that\n",
      "is\n",
      "easier\n",
      "to\n",
      "pack\n",
      "because\n",
      "liquid\n",
      "water\n",
      "molecules\n",
      "help\n",
      "snowflakes\n",
      "stick\n",
      "to\n",
      "each\n",
      "other.\n",
      "melted\n",
      "snow\n",
      "also\n",
      "plays\n",
      "a\n",
      "critical\n",
      "role\n",
      "in\n",
      "another\n",
      "windy\n",
      "activity,\n",
      "<0>\n",
      "Completely\n",
      "dry\n",
      "snow\n",
      "is\n",
      "very\n",
      "difficult\n",
      "to\n",
      "ski\n",
      "on\n",
      "because\n",
      "there's\n",
      "too\n",
      "much\n",
      "friction\n",
      "between\n",
      "the\n",
      "jagged\n",
      "snowflakes\n",
      "and\n",
      "the\n",
      "ski\n",
      "surface.\n",
      "So\n",
      "what's\n",
      "happening\n",
      "is\n",
      "that\n",
      "as\n",
      "skis\n",
      "move,\n",
      "they\n",
      "rub\n",
      "the\n",
      "surface\n",
      "of\n",
      "the\n",
      "snow\n",
      "and\n",
      "warm\n",
      "it\n",
      "up,\n",
      "creating\n",
      "a\n",
      "thin\n",
      "layer\n",
      "of\n",
      "water,\n",
      "which\n",
      "helps\n",
      "them\n",
      "\n",
      "L:\n",
      "Life's\n",
      "third\n",
      "act\n",
      "Jane\n",
      "Fonda\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "Life's\n",
      "third\n",
      "act\n",
      "Jane\n",
      "Fonda\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "third\n",
      "act\n",
      "the\n",
      "last\n",
      "three\n",
      "decades\n",
      "of\n",
      "life\n",
      "they\n",
      "realize\n",
      "that\n",
      "this\n",
      "is\n",
      "actually\n",
      "a\n",
      "developmental\n",
      "stage\n",
      "of\n",
      "life\n",
      "with\n",
      "its\n",
      "own\n",
      "significance\n",
      "as\n",
      "different\n",
      "from\n",
      "midlife\n",
      "as\n",
      "<0>\n",
      "from\n",
      "childhood\n",
      "and\n",
      "they\n",
      "are\n",
      "asking\n",
      "we\n",
      "we\n",
      "should\n",
      "all\n",
      "be\n",
      "asking\n",
      "how\n",
      "do\n",
      "we\n",
      "use\n",
      "this\n",
      "time\n",
      "how\n",
      "do\n",
      "we\n",
      "live\n",
      "it\n",
      "successfully\n",
      "what\n",
      "is\n",
      "the\n",
      "appropriate\n",
      "new\n",
      "metaphor\n",
      "for\n",
      "aging\n",
      "I've\n",
      "spent\n",
      "the\n",
      "last\n",
      "year\n",
      "researching\n",
      "and\n",
      "writing\n",
      "about\n",
      "this\n",
      "subject\n",
      "and\n",
      "I\n",
      "have\n",
      "come\n",
      "to\n",
      "find\n",
      "that\n",
      "a\n",
      "more\n",
      "appropriate\n",
      "metaphor\n",
      "for\n",
      "aging\n",
      "is\n",
      "a\n",
      "staircase\n",
      "the\n",
      "upward\n",
      "resurrection\n",
      "of\n",
      "the\n",
      "human\n",
      "spirit\n",
      "bringing\n",
      "us\n",
      "into\n",
      "wisdom\n",
      "wholeness\n",
      "\n",
      "L:\n",
      "Click\n",
      "Your\n",
      "Fortune\n",
      "3\n",
      "If\n",
      "you\n",
      "gave\n",
      "a\n",
      "TED\n",
      "Talk\n",
      "Francis\n",
      "de\n",
      "los\n",
      "Reyes\n",
      "III\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "Click\n",
      "Your\n",
      "Fortune\n",
      "3\n",
      "If\n",
      "you\n",
      "gave\n",
      "a\n",
      "TED\n",
      "Talk\n",
      "Francis\n",
      "de\n",
      "los\n",
      "Reyes\n",
      "III\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "in\n",
      "the\n",
      "developing\n",
      "world.\n",
      "And\n",
      "I\n",
      "think\n",
      "if\n",
      "I\n",
      "were\n",
      "to\n",
      "give\n",
      "a\n",
      "TED\n",
      "Talk,\n",
      "I\n",
      "would\n",
      "focus\n",
      "on\n",
      "that,\n",
      "on\n",
      "the\n",
      "2.5\n",
      "billion\n",
      "people\n",
      "who\n",
      "don't\n",
      "have\n",
      "access\n",
      "to\n",
      "sanitation\n",
      "and\n",
      "try\n",
      "to\n",
      "show\n",
      "solutions\n",
      "and\n",
      "also\n",
      "how\n",
      "we\n",
      "should\n",
      "change\n",
      "how\n",
      "we\n",
      "think\n",
      "about\n",
      "this\n",
      "problem\n",
      "because\n",
      "it's\n",
      "really\n",
      "a\n",
      "big,\n",
      "big\n",
      "problem\n",
      "in\n",
      "the\n",
      "world.\n",
      "Click\n",
      "any\n",
      "of\n",
      "these\n",
      "fortune\n",
      "cookies\n",
      "to\n",
      "see\n",
      "your\n",
      "questions\n",
      "and\n",
      "follow-up\n",
      "questions\n",
      "explored.\n",
      "Click\n",
      "this\n",
      "cookie\n",
      "to\n",
      "return\n",
      "to\n",
      "the\n",
      "intro\n",
      "video\n",
      "and\n",
      "see\n",
      "what\n",
      "this\n",
      "series\n",
      "is\n",
      "all\n",
      "about,\n",
      "or\n",
      "click\n",
      "this\n",
      "cookie\n",
      "to\n",
      "suggest\n",
      "alternative\n",
      "questions,\n",
      "participants,\n",
      "or\n",
      "career\n",
      "\n",
      "L:\n",
      "Passion\n",
      "for\n",
      "performance\n",
      "donner\n",
      "<0>\n",
      "TEDxHarderwijk\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "Passion\n",
      "for\n",
      "performance\n",
      "donner\n",
      "<3>\n",
      "TEDxHarderwijk\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "en\n",
      "het\n",
      "was\n",
      "<4>\n",
      "wel\n",
      "u\n",
      "<5>\n",
      "voor\n",
      "champion\n",
      "and\n",
      "and\n",
      "and\n",
      "bigger\n",
      "the\n",
      "price\n",
      "of\n",
      "course\n",
      "and\n",
      "then\n",
      "haar\n",
      "started\n",
      "to\n",
      "realize\n",
      "what\n",
      "not\n",
      "really\n",
      "happened\n",
      "because\n",
      "it\n",
      "was\n",
      "a\n",
      "bit\n",
      "and\n",
      "conscious\n",
      "of\n",
      "course\n",
      "a\n",
      "shelter\n",
      "snapchat\n",
      "wat\n",
      "<6>\n",
      "never\n",
      "seen\n",
      "this\n",
      "very\n",
      "talented\n",
      "and\n",
      "Marco\n",
      "<7>\n",
      "en\n",
      "life-size\n",
      "bon\n",
      "Torosaurus\n",
      "kan\n",
      "nou\n",
      "<8>\n",
      "je\n",
      "<9>\n",
      "word\n",
      "de\n",
      "je\n",
      "af\n",
      "te\n",
      "<10>\n",
      "er\n",
      "20\n",
      "seconds\n",
      "and\n",
      "travels\n",
      "of\n",
      "<1>\n",
      "<11>\n",
      "dat\n",
      "er\n",
      "Deah\n",
      "belt\n",
      "en\n",
      "god\n",
      "<12>\n",
      "over\n",
      "het\n",
      "stil\n",
      "de\n",
      "<13>\n",
      "voor\n",
      "swing\n",
      "swing\n",
      "van\n",
      "what\n",
      "happened\n",
      "so\n",
      "big\n",
      "world\n",
      "champion\n",
      "over\n",
      "<0>\n",
      "en\n",
      "<2>\n",
      "een\n",
      "week\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_gen(gen(X_train, Y_train, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "seeking\n",
      "the\n",
      "Truth\n",
      "in\n",
      "Human\n",
      "trafficking\n",
      "and\n",
      "other\n",
      "things\n",
      "too\n",
      "John\n",
      "<0>\n",
      "TEDxCarsonCity\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "seeking\n",
      "slam\n",
      "Truth\n",
      "in\n",
      "Human\n",
      "trafficking\n",
      "and\n",
      "other\n",
      "slam\n",
      "too\n",
      "John\n",
      "<0>\n",
      "TEDxCarsonCity\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "right\n",
      "or\n",
      "wrong\n",
      "good\n",
      "or\n",
      "bad\n",
      "just\n",
      "or\n",
      "unjust\n",
      "true\n",
      "or\n",
      "false\n",
      "they\n",
      "they\n",
      "try\n",
      "to\n",
      "paint\n",
      "us\n",
      "into\n",
      "corners\n",
      "where\n",
      "seemingly\n",
      "our\n",
      "only\n",
      "choices\n",
      "are\n",
      "to\n",
      "either\n",
      "completely\n",
      "agree\n",
      "or\n",
      "completely\n",
      "disagree\n",
      "they\n",
      "neglect\n",
      "to\n",
      "tell\n",
      "us\n",
      "about\n",
      "the\n",
      "good\n",
      "work\n",
      "that\n",
      "happens\n",
      "when\n",
      "people\n",
      "meet\n",
      "in\n",
      "the\n",
      "middle\n",
      "and\n",
      "share\n",
      "their\n",
      "passion\n",
      "and\n",
      "their\n",
      "knowledge\n",
      "and\n",
      "we're\n",
      "real\n",
      "effective\n",
      "change\n",
      "can\n",
      "happen\n",
      "so\n",
      "I\n",
      "would\n",
      "urge\n",
      "you\n",
      "if\n",
      "any\n",
      "of\n",
      "these\n",
      "important\n",
      "issues\n",
      "that\n",
      "we're\n",
      "talking\n",
      "about\n",
      "many\n",
      "of\n",
      "which\n",
      "we\n",
      "are\n",
      "talking\n",
      "about\n",
      "today\n",
      "if\n",
      "they\n",
      "resonate\n",
      "within\n",
      "you\n",
      "embrace\n",
      "their\n",
      "complexities\n",
      "study\n",
      "them\n",
      "really\n",
      "learn\n",
      "talk\n",
      "\n",
      "L:\n",
      "The\n",
      "science\n",
      "of\n",
      "snowflakes\n",
      "<0>\n",
      "<1>\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "The\n",
      "Whale\n",
      "of\n",
      "snowflakes\n",
      "<1>\n",
      "<2>\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "are\n",
      "similar,\n",
      "a\n",
      "symmetric\n",
      "snowflake\n",
      "can\n",
      "grow.\n",
      "Weather\n",
      "conditions\n",
      "affect\n",
      "snow\n",
      "on\n",
      "the\n",
      "ground,\n",
      "as\n",
      "well.\n",
      "warmer\n",
      "ground\n",
      "temperatures\n",
      "produce\n",
      "a\n",
      "colder\n",
      "snow\n",
      "that\n",
      "is\n",
      "easier\n",
      "to\n",
      "pack\n",
      "because\n",
      "liquid\n",
      "water\n",
      "molecules\n",
      "help\n",
      "snowflakes\n",
      "stick\n",
      "to\n",
      "each\n",
      "other.\n",
      "melted\n",
      "snow\n",
      "also\n",
      "plays\n",
      "a\n",
      "critical\n",
      "role\n",
      "in\n",
      "another\n",
      "windy\n",
      "activity,\n",
      "<0>\n",
      "Completely\n",
      "dry\n",
      "snow\n",
      "is\n",
      "very\n",
      "difficult\n",
      "to\n",
      "ski\n",
      "on\n",
      "because\n",
      "there's\n",
      "too\n",
      "much\n",
      "friction\n",
      "between\n",
      "the\n",
      "jagged\n",
      "snowflakes\n",
      "and\n",
      "the\n",
      "ski\n",
      "surface.\n",
      "So\n",
      "what's\n",
      "happening\n",
      "is\n",
      "that\n",
      "as\n",
      "skis\n",
      "move,\n",
      "they\n",
      "rub\n",
      "the\n",
      "surface\n",
      "of\n",
      "the\n",
      "snow\n",
      "and\n",
      "warm\n",
      "it\n",
      "up,\n",
      "creating\n",
      "a\n",
      "thin\n",
      "layer\n",
      "of\n",
      "water,\n",
      "which\n",
      "helps\n",
      "them\n",
      "\n",
      "L:\n",
      "Life's\n",
      "third\n",
      "act\n",
      "Jane\n",
      "Fonda\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "Life's\n",
      "third\n",
      "act\n",
      "slam\n",
      "Fonda\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "third\n",
      "act\n",
      "the\n",
      "last\n",
      "three\n",
      "decades\n",
      "of\n",
      "life\n",
      "they\n",
      "realize\n",
      "that\n",
      "this\n",
      "is\n",
      "actually\n",
      "a\n",
      "developmental\n",
      "stage\n",
      "of\n",
      "life\n",
      "with\n",
      "its\n",
      "own\n",
      "significance\n",
      "as\n",
      "different\n",
      "from\n",
      "midlife\n",
      "as\n",
      "<0>\n",
      "from\n",
      "childhood\n",
      "and\n",
      "they\n",
      "are\n",
      "asking\n",
      "we\n",
      "we\n",
      "should\n",
      "all\n",
      "be\n",
      "asking\n",
      "how\n",
      "do\n",
      "we\n",
      "use\n",
      "this\n",
      "time\n",
      "how\n",
      "do\n",
      "we\n",
      "live\n",
      "it\n",
      "successfully\n",
      "what\n",
      "is\n",
      "the\n",
      "appropriate\n",
      "new\n",
      "metaphor\n",
      "for\n",
      "aging\n",
      "I've\n",
      "spent\n",
      "the\n",
      "last\n",
      "year\n",
      "researching\n",
      "and\n",
      "writing\n",
      "about\n",
      "this\n",
      "subject\n",
      "and\n",
      "I\n",
      "have\n",
      "come\n",
      "to\n",
      "find\n",
      "that\n",
      "a\n",
      "more\n",
      "appropriate\n",
      "metaphor\n",
      "for\n",
      "aging\n",
      "is\n",
      "a\n",
      "staircase\n",
      "the\n",
      "upward\n",
      "resurrection\n",
      "of\n",
      "the\n",
      "human\n",
      "spirit\n",
      "bringing\n",
      "us\n",
      "into\n",
      "wisdom\n",
      "wholeness\n",
      "\n",
      "L:\n",
      "Click\n",
      "Your\n",
      "Fortune\n",
      "3\n",
      "If\n",
      "you\n",
      "gave\n",
      "a\n",
      "TED\n",
      "Talk\n",
      "Francis\n",
      "de\n",
      "los\n",
      "Reyes\n",
      "III\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "Click\n",
      "Your\n",
      "slam\n",
      "slam\n",
      "If\n",
      "you\n",
      "gave\n",
      "slam\n",
      "TED\n",
      "Talk\n",
      "Francis\n",
      "slam\n",
      "los\n",
      "Reyes\n",
      "III\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "in\n",
      "the\n",
      "developing\n",
      "world.\n",
      "And\n",
      "I\n",
      "think\n",
      "if\n",
      "I\n",
      "were\n",
      "to\n",
      "give\n",
      "a\n",
      "TED\n",
      "Talk,\n",
      "I\n",
      "would\n",
      "focus\n",
      "on\n",
      "that,\n",
      "on\n",
      "the\n",
      "2.5\n",
      "billion\n",
      "people\n",
      "who\n",
      "don't\n",
      "have\n",
      "access\n",
      "to\n",
      "sanitation\n",
      "and\n",
      "try\n",
      "to\n",
      "show\n",
      "solutions\n",
      "and\n",
      "also\n",
      "how\n",
      "we\n",
      "should\n",
      "change\n",
      "how\n",
      "we\n",
      "think\n",
      "about\n",
      "this\n",
      "problem\n",
      "because\n",
      "it's\n",
      "really\n",
      "a\n",
      "big,\n",
      "big\n",
      "problem\n",
      "in\n",
      "the\n",
      "world.\n",
      "Click\n",
      "any\n",
      "of\n",
      "these\n",
      "fortune\n",
      "cookies\n",
      "to\n",
      "see\n",
      "your\n",
      "questions\n",
      "and\n",
      "follow-up\n",
      "questions\n",
      "explored.\n",
      "Click\n",
      "this\n",
      "cookie\n",
      "to\n",
      "return\n",
      "to\n",
      "the\n",
      "intro\n",
      "video\n",
      "and\n",
      "see\n",
      "what\n",
      "this\n",
      "series\n",
      "is\n",
      "all\n",
      "about,\n",
      "or\n",
      "click\n",
      "this\n",
      "cookie\n",
      "to\n",
      "suggest\n",
      "alternative\n",
      "questions,\n",
      "participants,\n",
      "or\n",
      "career\n",
      "\n",
      "L:\n",
      "Passion\n",
      "for\n",
      "performance\n",
      "donner\n",
      "<0>\n",
      "TEDxHarderwijk\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "Passion\n",
      "800\n",
      "performance\n",
      "donner\n",
      "<3>\n",
      "TEDxHarderwijk\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "en\n",
      "het\n",
      "was\n",
      "<4>\n",
      "wel\n",
      "u\n",
      "<5>\n",
      "voor\n",
      "champion\n",
      "and\n",
      "and\n",
      "and\n",
      "bigger\n",
      "the\n",
      "price\n",
      "of\n",
      "course\n",
      "and\n",
      "then\n",
      "haar\n",
      "started\n",
      "to\n",
      "realize\n",
      "what\n",
      "not\n",
      "really\n",
      "happened\n",
      "because\n",
      "it\n",
      "was\n",
      "a\n",
      "bit\n",
      "and\n",
      "conscious\n",
      "of\n",
      "course\n",
      "a\n",
      "shelter\n",
      "snapchat\n",
      "wat\n",
      "<6>\n",
      "never\n",
      "seen\n",
      "this\n",
      "very\n",
      "talented\n",
      "and\n",
      "Marco\n",
      "<7>\n",
      "en\n",
      "life-size\n",
      "bon\n",
      "Torosaurus\n",
      "kan\n",
      "nou\n",
      "<8>\n",
      "je\n",
      "<9>\n",
      "word\n",
      "de\n",
      "je\n",
      "af\n",
      "te\n",
      "<10>\n",
      "er\n",
      "20\n",
      "seconds\n",
      "and\n",
      "travels\n",
      "of\n",
      "<1>\n",
      "<11>\n",
      "dat\n",
      "er\n",
      "Deah\n",
      "belt\n",
      "en\n",
      "god\n",
      "<12>\n",
      "over\n",
      "het\n",
      "stil\n",
      "de\n",
      "<13>\n",
      "voor\n",
      "swing\n",
      "swing\n",
      "van\n",
      "what\n",
      "happened\n",
      "so\n",
      "big\n",
      "world\n",
      "champion\n",
      "over\n",
      "<0>\n",
      "en\n",
      "<2>\n",
      "een\n",
      "week\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_gen(gen(X_train, Y_train, nflips=6, model=model, debug=False, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "valgen = gen(X_test, Y_test,nb_batches=3, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "How\n",
      "I\n",
      "responded\n",
      "to\n",
      "sexism\n",
      "in\n",
      "gaming\n",
      "with\n",
      "empathy\n",
      "Marcel\n",
      "Chen\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "How\n",
      "I\n",
      "responded\n",
      "to\n",
      "sexism\n",
      "in\n",
      "gaming\n",
      "with\n",
      "empathy\n",
      "Marcel\n",
      "Chen\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "like\n",
      "<0>\n",
      "she\n",
      "just\n",
      "wants\n",
      "<1>\n",
      "And\n",
      "then\n",
      "you\n",
      "started\n",
      "to\n",
      "see\n",
      "comments\n",
      "like\n",
      "this.\n",
      "<2>\n",
      "you're\n",
      "only\n",
      "known\n",
      "in\n",
      "the\n",
      "scene\n",
      "for\n",
      "being\n",
      "the\n",
      "subject\n",
      "of\n",
      "nerdy\n",
      "<3>\n",
      "<4>\n",
      "a\n",
      "<5>\n",
      "in\n",
      "crappy\n",
      "<6>\n",
      "<7>\n",
      "Over\n",
      "years,\n",
      "I\n",
      "began\n",
      "internalizing\n",
      "all\n",
      "of\n",
      "this,\n",
      "and\n",
      "then\n",
      "I\n",
      "took\n",
      "these\n",
      "attitudes\n",
      "and\n",
      "projected\n",
      "them\n",
      "onto\n",
      "other\n",
      "women.\n",
      "<8>\n",
      "why\n",
      "is\n",
      "she\n",
      "so\n",
      "<9>\n",
      "Is\n",
      "she\n",
      "even\n",
      "a\n",
      "real\n",
      "<10>\n",
      "I\n",
      "felt\n",
      "my\n",
      "voice\n",
      "shrinking\n",
      "and\n",
      "the\n",
      "resent\n",
      "growing\n",
      "inside\n",
      "of\n",
      "me,\n",
      "and\n",
      "eventually,\n",
      "I\n",
      "distanced\n",
      "myself\n",
      "from\n",
      "the\n",
      "Smash\n",
      "community\n",
      "altogether.\n",
      "Fast\n",
      "forward\n",
      "a\n",
      "few\n",
      "years.\n",
      "I\n",
      "landed\n",
      "my\n",
      "first\n",
      "job\n",
      "\n",
      "L:\n",
      "How\n",
      "to\n",
      "stay\n",
      "calm\n",
      "when\n",
      "you\n",
      "know\n",
      "you'll\n",
      "be\n",
      "stressed\n",
      "Daniel\n",
      "<0>\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "How\n",
      "to\n",
      "stay\n",
      "calm\n",
      "when\n",
      "you\n",
      "know\n",
      "you'll\n",
      "be\n",
      "stressed\n",
      "Daniel\n",
      "<3>\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "and\n",
      "windows,\n",
      "and\n",
      "they\n",
      "were\n",
      "locked\n",
      "<1>\n",
      "I\n",
      "thought\n",
      "about\n",
      "calling\n",
      "a\n",
      "janitor\n",
      "\n",
      "at\n",
      "least\n",
      "I\n",
      "had\n",
      "my\n",
      "cellphone,\n",
      "but\n",
      "at\n",
      "midnight,\n",
      "it\n",
      "could\n",
      "take\n",
      "a\n",
      "while\n",
      "for\n",
      "a\n",
      "janitor\n",
      "to\n",
      "show\n",
      "up,\n",
      "and\n",
      "it\n",
      "was\n",
      "cold.\n",
      "I\n",
      "couldn't\n",
      "go\n",
      "back\n",
      "to\n",
      "my\n",
      "friend\n",
      "<2>\n",
      "house\n",
      "for\n",
      "the\n",
      "night\n",
      "because\n",
      "I\n",
      "had\n",
      "an\n",
      "early\n",
      "flight\n",
      "to\n",
      "Europe\n",
      "the\n",
      "next\n",
      "morning,\n",
      "and\n",
      "I\n",
      "needed\n",
      "to\n",
      "get\n",
      "my\n",
      "passport\n",
      "and\n",
      "my\n",
      "<0>\n",
      "So,\n",
      "desperate\n",
      "and\n",
      "freezing\n",
      "cold,\n",
      "I\n",
      "found\n",
      "a\n",
      "large\n",
      "rock\n",
      "and\n",
      "I\n",
      "broke\n",
      "through\n",
      "the\n",
      "basement\n",
      "window,\n",
      "cleared\n",
      "out\n",
      "the\n",
      "shards\n",
      "of\n",
      "glass,\n",
      "I\n",
      "crawled\n",
      "through,\n",
      "I\n",
      "found\n",
      "a\n",
      "\n",
      "L:\n",
      "My\n",
      "Journey\n",
      "Of\n",
      "Enlightenment\n",
      "Janine\n",
      "<0>\n",
      "TEDxIIFTDelhi\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "My\n",
      "Journey\n",
      "Of\n",
      "Enlightenment\n",
      "Janine\n",
      "<0>\n",
      "TEDxIIFTDelhi\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "lead\n",
      "to\n",
      "enlightenment\n",
      "the\n",
      "third\n",
      "pillar\n",
      "of\n",
      "life\n",
      "enlightenment\n",
      "is\n",
      "the\n",
      "ability\n",
      "to\n",
      "perceive\n",
      "the\n",
      "truth\n",
      "it\n",
      "is\n",
      "also\n",
      "the\n",
      "ability\n",
      "to\n",
      "analyze\n",
      "a\n",
      "situation\n",
      "and\n",
      "tread\n",
      "cautious\n",
      "towards\n",
      "your\n",
      "action\n",
      "plan\n",
      "I\n",
      "would\n",
      "be\n",
      "happy\n",
      "if\n",
      "my\n",
      "experience\n",
      "serves\n",
      "as\n",
      "an\n",
      "instrument\n",
      "towards\n",
      "your\n",
      "enlightenment\n",
      "nothing\n",
      "is\n",
      "impossible\n",
      "is\n",
      "the\n",
      "inside\n",
      "that\n",
      "dawned\n",
      "on\n",
      "me\n",
      "that\n",
      "day\n",
      "in\n",
      "closing\n",
      "I\n",
      "would\n",
      "like\n",
      "to\n",
      "read\n",
      "something\n",
      "I'd\n",
      "like\n",
      "to\n",
      "share\n",
      "something\n",
      "that\n",
      "I\n",
      "read\n",
      "online\n",
      "I\n",
      "don't\n",
      "know\n",
      "the\n",
      "author\n",
      "but\n",
      "I\n",
      "found\n",
      "it\n",
      "quite\n",
      "interesting\n",
      "an\n",
      "app\n",
      "for\n",
      "me\n",
      "it's\n",
      "titled\n",
      "God\n",
      "assigning\n",
      "duties\n",
      "to\n",
      "each\n",
      "zodiac\n",
      "sign\n",
      "to\n",
      "\n",
      "L:\n",
      "How\n",
      "I\n",
      "responded\n",
      "to\n",
      "sexism\n",
      "in\n",
      "gaming\n",
      "with\n",
      "empathy\n",
      "Marcel\n",
      "Chen\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "H:\n",
      "~\n",
      "How\n",
      "I\n",
      "responded\n",
      "to\n",
      "sexism\n",
      "in\n",
      "gaming\n",
      "with\n",
      "empathy\n",
      "Marcel\n",
      "Chen\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "\n",
      "D:\n",
      "like\n",
      "<0>\n",
      "she\n",
      "just\n",
      "wants\n",
      "<1>\n",
      "And\n",
      "then\n",
      "you\n",
      "started\n",
      "to\n",
      "see\n",
      "comments\n",
      "like\n",
      "this.\n",
      "<2>\n",
      "you're\n",
      "only\n",
      "known\n",
      "in\n",
      "the\n",
      "scene\n",
      "for\n",
      "being\n",
      "the\n",
      "subject\n",
      "of\n",
      "nerdy\n",
      "<3>\n",
      "<4>\n",
      "a\n",
      "<5>\n",
      "in\n",
      "crappy\n",
      "<6>\n",
      "<7>\n",
      "Over\n",
      "years,\n",
      "I\n",
      "began\n",
      "internalizing\n",
      "all\n",
      "of\n",
      "this,\n",
      "and\n",
      "then\n",
      "I\n",
      "took\n",
      "these\n",
      "attitudes\n",
      "and\n",
      "projected\n",
      "them\n",
      "onto\n",
      "other\n",
      "women.\n",
      "<8>\n",
      "why\n",
      "is\n",
      "she\n",
      "so\n",
      "<9>\n",
      "Is\n",
      "she\n",
      "even\n",
      "a\n",
      "real\n",
      "<10>\n",
      "I\n",
      "felt\n",
      "my\n",
      "voice\n",
      "shrinking\n",
      "and\n",
      "the\n",
      "resent\n",
      "growing\n",
      "inside\n",
      "of\n",
      "me,\n",
      "and\n",
      "eventually,\n",
      "I\n",
      "distanced\n",
      "myself\n",
      "from\n",
      "the\n",
      "Smash\n",
      "community\n",
      "altogether.\n",
      "Fast\n",
      "forward\n",
      "a\n",
      "few\n",
      "years.\n",
      "I\n",
      "landed\n",
      "my\n",
      "first\n",
      "job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(4):\n",
    "    test_gen(valgen, n=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflips=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = gen(X_train, Y_train, batch_size=batch_size, nflips=nflips, model=model)\n",
    "valgen = gen(X_test, Y_test, nb_batches=nb_val_samples//batch_size, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 125), (64, 25, 40000), 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = next(traingen)\n",
    "r[0].shape, r[1].shape, len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-449905357d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     h = model.fit_generator(traingen, steps_per_epoch=nb_train_samples//batch_size,\n\u001b[1;32m      4\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                            )\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2080\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2081\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    997\u001b[0m                                                  \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                                                  **self._function_kwargs)\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Invalid argument \"%s\" passed to K.function with Theano backend'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   1232\u001b[0m                                         \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                                         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m                                         **kwargs)\n\u001b[0m\u001b[1;32m   1235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    315\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1837\u001b[0m                   \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m                   \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys, name)\u001b[0m\n\u001b[1;32m   1517\u001b[0m                         optimizer, inputs, outputs)\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m                     \u001b[0moptimizer_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0mnb_nodes_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                     \u001b[0msub_prof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                     \u001b[0msub_profs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph, start_from)\u001b[0m\n\u001b[1;32m   2511\u001b[0m                         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2512\u001b[0m                         \u001b[0mt_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2513\u001b[0;31m                         \u001b[0mlopt_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2514\u001b[0m                         \u001b[0mtime_opts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlopt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2515\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlopt_change\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(self, fgraph, node, lopt)\u001b[0m\n\u001b[1;32m   2032\u001b[0m         \u001b[0mlopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlopt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m             \u001b[0mreplacements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2035\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailure_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/tensor/opt.py\u001b[0m in \u001b[0;36mlocal_subtensor_merge\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m   3089\u001b[0m                                          \u001b[0mxshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3090\u001b[0m                                          \u001b[0mslices2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3091\u001b[0;31m                                          ushape[pos_2]))\n\u001b[0m\u001b[1;32m   3092\u001b[0m                     \u001b[0mpos_2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/tensor/opt.py\u001b[0m in \u001b[0;36mmerge_two_slices\u001b[0;34m(slice1, len1, slice2, len2)\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_greedy_local_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_greedy_local_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3040\u001b[0;31m         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_greedy_local_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3041\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_greedy_local_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mpre_greedy_local_optimizer\u001b[0;34m(list_optimizations, out)\u001b[0m\n\u001b[1;32m   2921\u001b[0m         \u001b[0mout_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m     final_outs, optimized_nodes = local_recursive_function(\n\u001b[0;32m-> 2923\u001b[0;31m         list_optimizations, out, {}, 0)\n\u001b[0m\u001b[1;32m   2924\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mlocal_recursive_function\u001b[0;34m(list_opt, out, optimized_vars, depth)\u001b[0m\n\u001b[1;32m   2893\u001b[0m                         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                         depth + 1)\n\u001b[0m\u001b[1;32m   2896\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mlocal_recursive_function\u001b[0;34m(list_opt, out, optimized_vars, depth)\u001b[0m\n\u001b[1;32m   2893\u001b[0m                         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                         depth + 1)\n\u001b[0m\u001b[1;32m   2896\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mlocal_recursive_function\u001b[0;34m(list_opt, out, optimized_vars, depth)\u001b[0m\n\u001b[1;32m   2893\u001b[0m                         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                         depth + 1)\n\u001b[0m\u001b[1;32m   2896\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mlocal_recursive_function\u001b[0;34m(list_opt, out, optimized_vars, depth)\u001b[0m\n\u001b[1;32m   2893\u001b[0m                         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                         depth + 1)\n\u001b[0m\u001b[1;32m   2896\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mlocal_recursive_function\u001b[0;34m(list_opt, out, optimized_vars, depth)\u001b[0m\n\u001b[1;32m   2893\u001b[0m                         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                         depth + 1)\n\u001b[0m\u001b[1;32m   2896\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mlocal_recursive_function\u001b[0;34m(list_opt, out, optimized_vars, depth)\u001b[0m\n\u001b[1;32m   2893\u001b[0m                         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                         depth + 1)\n\u001b[0m\u001b[1;32m   2896\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                         \u001b[0moptimized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mlocal_recursive_function\u001b[0;34m(list_opt, out, optimized_vars, depth)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2906\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_opt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2907\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2908\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/tensor/opt.py\u001b[0m in \u001b[0;36mconstant_folding\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m   6507\u001b[0m                                no_recycling=[], impl=impl)\n\u001b[1;32m   6508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6509\u001b[0;31m     \u001b[0mrequired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6510\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrequired\u001b[0m  \u001b[0;31m# a node whose inputs are all provided should always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6511\u001b[0m     \u001b[0;31m# return successfully\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/tensor/elemwise.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inputs, output_storage)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mout_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mout_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mufunc_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(5):\n",
    "    print ('Iteration', iteration)\n",
    "    h = model.fit_generator(traingen, steps_per_epoch=nb_train_samples//batch_size,\n",
    "                        epochs=1, validation_data=valgen, validation_steps=nb_val_samples//batch_size,\n",
    "                            use_multiprocessing=True\n",
    "                           )\n",
    "    for k,v in h.history.iteritems():\n",
    "        history[k] = history.get(k,[]) + v\n",
    "    with open('data/%s.history.pkl'%FN,'wb') as fp:\n",
    "        pickle.dump(history,fp,-1)\n",
    "    model.save_weights('data/%s.hdf5'%FN, overwrite=True)\n",
    "    gensamples(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
